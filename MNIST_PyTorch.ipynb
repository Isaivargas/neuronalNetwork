{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMg/eftyki0FHdq7KBTFWiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaivargas/neuronalNetwork/blob/master/MNIST_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keORhJXyiK9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "31481fb0-23fe-46eb-f6eb-964191bc825a"
      },
      "source": [
        "!pip install matplotlib \n",
        "!pip install numpy\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maGhFhXDizTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f65a95a3-0c1a-4bf2-f463-75279b4cf8ca"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import helper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "\n",
        "from torchvision import datasets , transforms\n",
        "\n",
        "#define a transform to normalize the data \n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#Download and load the trainning data \n",
        "trainset = datasets.MNIST('MNIST_data/',download = True ,train = True,transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images ,labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# Sigmoid function activation\n",
        "def activation(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "\n",
        "inputs = images.view(images.shape[0],-1)\n",
        "\n",
        "w1 = torch.randn(784,256)\n",
        "b1 = torch.randn(256)\n",
        "\n",
        "w2 = torch.randn(256,10)\n",
        "b2 = torch.randn(10)\n",
        "\n",
        "h  = activation(torch.mm(inputs,w1) + b1)\n",
        "output = torch.mm(h,w2) + b2\n",
        "\n",
        "def softmax(x):\n",
        "    return torch.exp(x)/torch.sum(torch.exp(x),dim=1).view(-1,1)\n",
        "\n",
        "probabilities = softmax(output)\n",
        "\n",
        "print(probabilities.shape)\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    #Inputs to hidden layer linear transformation\n",
        "    self.hidden = nn.Linear(784,256)\n",
        "    #Output layer 10 units one for each digit\n",
        "    self.output = nn.Linear(256,10)\n",
        "\n",
        "    #Define sigmoid activation and softmaxOutput\n",
        "    self.sigmoid = nn.Sigmoid\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      #Pass the input tensor through each of four operations\n",
        "      x = self.hidden(x) #Hidden layer\n",
        "      x = self.sigmoid(x) #Output Layer\n",
        "      x = self.output(x)\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x  \n",
        "\n",
        "class Network2(nn.Module):\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    def __init__(self):\n",
        "       super().__init__()\n",
        "\n",
        "       # Inputs to hidden layer linear transformation\n",
        "       # nn.Linear(inputTensorShape,outPutTensorShape)\n",
        "       self.hidden = nn.Linear(784,256)\n",
        "       #Output layer 10 units one for each digit\n",
        "       self.output = nn.Linear(256,10)\n",
        "\n",
        "       x = F.sigmoid (self.hidden(x))\n",
        "       x = F.softmax (self.output(x),dim=1)\n",
        "\n",
        "       return x \n",
        "\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784,128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1)\n",
        "                        )\n",
        "\n",
        "#Define the loss function\n",
        "criterion = nn.NLLLoss()\n",
        "#Define the optimizer \n",
        "optimizer = optim.SGD(model.parameters(),lr = 0.003)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images ,labels in trainloader:\n",
        "  #Flatten images\n",
        "      images = images.view(images.shape[0],-1)\n",
        "# Clear the gradients \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model.forward(images)\n",
        "      loss = criterion(output,labels) \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss +=loss.item()\n",
        "  else:\n",
        "      print(f\"Trainning Loss{running_loss/len(trainloader)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 10])\n",
            "Trainning Loss1.87967169869429\n",
            "Trainning Loss0.8495651853046438\n",
            "Trainning Loss0.530471018350709\n",
            "Trainning Loss0.4335635250914834\n",
            "Trainning Loss0.38775519189486374\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}